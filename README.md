# Federated Multi-Task Learning under a Mixture of Distributions

This repository is the official implementation of [Federated Multi-Task Learning under a Mixture of Distributions](https://arxiv.org/abs/2108.10252). 

 该存储库是 [Federated Multi-Task Learning under a Mixture of Distributions](https://arxiv.org/abs/2108.10252) 的官方实现。

 The increasing size of data generated by smartphones and IoT devices motivated the development of Federated Learning (FL), a framework for on-device collaborative training of machine learning models.  First efforts in FL focused on learning a single global model with good average  performance across clients, but the global model may be arbitrarily bad for a given client, due to the inherent heterogeneity of local data distributions.  Federated multi-task learning (MTL) approaches  can learn personalized models by  formulating an opportune penalized optimization problem. The penalization term can  capture complex relations among personalized models, but eschews clear statistical assumptions about local data distributions. In this work, we propose to study federated  MTL under the flexible assumption that each local data distribution is a mixture of unknown underlying distributions. 

智能手机和物联网设备生成的数据量不断增加推动了联邦学习 (FL) 的发展，联邦学习是一种用于机器学习模型的设备上协作训练的框架。 FL 的最初努力集中在学习一个单一的全局模型，该模型在客户端之间具有良好的平均性能，但由于本地数据分布的内在异质性，全局模型对于给定的客户端可能是任意糟糕的。 联合多任务学习 (MTL) 方法可以通过制定适当的惩罚优化问题来学习个性化模型。 惩罚项可以捕捉个性化模型之间的复杂关系，但避开关于局部数据分布的明确统计假设。 在这项工作中，我们建议在每个本地数据分布都是未知基础分布的混合的灵活假设下研究联邦 MTL。
 
This assumption encompasses most of the existing  personalized FL approaches and leads to federated EM-like algorithms for both  client-server and fully decentralized settings. Moreover, it provides a principled  way to serve personalized models to clients not seen at training time. The algorithms' convergence is analyzed through a novel federated surrogate optimization framework, which can be of general interest. Experimental results on FL benchmarks show that in most cases our approach provides models with higher accuracy and fairness  than state-of-the-art methods.

该假设包含大多数现有的个性化 FL 方法，并导致针对客户端-服务器和完全去中心化设置的联合 EM 类算法。 此外，它提供了一种原则性的方式来为培训时未见过的客户提供个性化模型。 算法的收敛性是通过一种新颖的联合代理优化框架进行分析的，这可能是普遍感兴趣的。 FL 基准测试的实验结果表明，在大多数情况下，我们的方法提供的模型比最先进的方法具有更高的准确性和公平性。

## Requirements


To install requirements:

```setup
pip install -r requirements.txt
```

## Usage

We provide code to simulate federated training of machine learning.  The core objects are `Aggregator` and `Client`, different federated learning algorithms can be implemented by revising the local update method  `Client.step()` and/or the aggregation protocol defined in `Aggregator.mix()` and `Aggregator.update_client()`. 


In addition to the trivial baseline consisting of training models locally without any collaboration, this repository supports the following federated learning algorithms:

* FedAvg ([McMahan et al. 2017](http://proceedings.mlr.press/v54/mcmahan17a.html))
* FedProx ([Li et al. 2018](https://arxiv.org/abs/1812.06127))
* Clustered FL ([Sattler et al. 2019](https://ieeexplore.ieee.org/abstract/document/9174890))
* pFedMe ([Dinh et al. 2020](https://proceedings.neurips.cc/paper/2020/file/f4f1f13c8289ac1b1ee0ff176b56fc60-Paper.pdf))
* L2SGD ([Hanzely et al. 2020](https://proceedings.neurips.cc/paper/2020/file/187acf7982f3c169b3075132380986e4-Paper.pdf))
* APFL ([Deng et al. 2020](https://arxiv.org/abs/2003.13461))
* q-FFL ([Li et al. 2020](https://openreview.net/forum?id=ByexElSYDr))
* AFL ([Mohri et al. 2019](http://proceedings.mlr.press/v97/mohri19a.html))

## Datasets

We provide five federated benchmark datasets spanning a wide range of machine learning tasks: image classification (CIFAR10 and CIFAR100), handwritten character recognition (EMNIST and FEMNIST), and language modelling (Shakespeare), in addition to a synthetic dataset 

我们提供五个联合基准数据集，涵盖广泛的机器学习任务：图像分类（CIFAR10 和 CIFAR100）、手写字符识别（EMNIST 和 FEMNIST）和语言建模（Shakespeare），以及一个合成数据集

Shakespeare dataset (resp. FEMNIST) was naturally partitioned by assigning all lines from the same characters (resp. all images from the same writer) to the same client.  We created federated versions of CIFAR10 and EMNIST by distributing samples with the same label across the clients according to a  symmetric Dirichlet distribution with parameter 0.4. For CIFAR100, we exploited the availability of "coarse" and "fine" labels, using a two-stage Pachinko allocation method  to assign 600 sample to each of the 100 clients.

Shakespeare数据集（resp. FEMNIST）通过将来自相同字符的所有行（resp. 来自同一作者的所有图像）分配给同一客户端来自然地划分。 我们创建了 CIFAR10 和 EMNIST 的联合版本，方法是根据参数为 0.4 的对称 Dirichlet 分布在客户端分配具有相同标签的样本。 对于 CIFAR100，我们利用“粗略”和“精细”标签的可用性，使用两阶段 Pachinko 分配方法为 100 个客户中的每个客户分配 600 个样本。

The following table summarizes the datasets and models

|Dataset         | Task |  Model |
| ------------------  |  ------|------- |
| FEMNIST   |     Handwritten character recognition       |     2-layer CNN + 2-layer FFN  |
| EMNIST    |    Handwritten character recognition     |      2-layer CNN + 2-layer FFN     |
| CIFAR10   |     Image classification        |      MobileNet-v2 |
| CIFAR100    |     Image classification         |      MobileNet-v2  |
| Shakespeare |     Next character prediction        |      Stacked LSTM    |
| Synthetic dataset| Binary classification | Linear model | 

See the `README.md` files of respective dataset, i.e., `data/$DATASET`,
for instructions on generating data

## Training

Run on one dataset, with a specific  choice of federated learning method. Specify the name of the dataset (experiment), the used method, and configure all other hyper-parameters (see all hyper-parameters values in the appendix of the paper)
在一个数据集上运行，并选择特定的联合学习方法。 指定数据集（实验）的名称，使用的方法，并配置所有其他超参数（参见论文附录中的所有超参数值）
```train
python3  python run_experiment.py cifar10 FedAvg \
    --n_learners 1 \
    --n_rounds 200 \
    --bz 128 \
    --lr 0.01 \
    --lr_scheduler multi_step \
    --log_freq 2 \
    --device cuda \
    --optimizer sgd \
    --seed 1234 \
    --logs_root ./logs \
    --verbose 1
```

The test and training accuracy and loss will be saved in the specified log path.

We provide example scripts to run paper experiments under `scripts/` directory.

## Evaluation

We give instructions to run experiments on CIFAR-10 dataset as an example (the same holds for the other datasets). You need first to go to  `./data/cifar10`, follow the instructions in `README.md` to download and partition the dataset.

All experiments will generate tensorboard log files (`logs/cifar10`) that you can interact with, using [TensorBoard](https://www.tensorflow.org/tensorboard)

### Average performance of personalized models

Run the following scripts, this will generate tensorboard logs that you can interact with to make plots or get the values presented in Table 2

```eval
# run FedAvg
echo "Run FedAvg"
python run_experiment.py cifar10 FedAvg --n_learners 1 --n_rounds 200 --bz 128 --lr 0.01 \
 --lr_scheduler multi_step --log_freq 2 --device cuda --optimizer sgd --seed 1234 --verbose 1

# run FedAvg + local adaption
echo "run FedAvg + local adaption"
python run_experiment.py cifar10 FedAvg --n_learners 1 --locally_tune_clients --n_rounds 201 --bz 128 \
 --lr 0.001 --lr_scheduler multi_step --log_freq 10 --device cuda --optimizer sgd --seed 1234 --verbose 1

# run training using local data only
echo "Run Local"
python run_experiment.py cifar10 local --n_learners 1 --n_rounds 201 --bz 128 --lr 0.03 \
 --lr_scheduler multi_step --log_freq 10 --device cuda --optimizer sgd --seed 1234 --verbose 1

# run Clustered FL
echo "Run Clustered FL"
python run_experiment.py cifar10 clustered --n_learners 1 --n_rounds 201 --bz 128 --lr 0.003 \
 --lr_scheduler multi_step --log_freq 10 --device cuda --optimizer sgd --seed 1234 --verbose 1

# run FedProx
echo "Run FedProx"
python run_experiment.py cifar10 FedProx --n_learners 1 --n_rounds 201 --bz 128 --lr 0.01 --mu 1.0\
 --lr_scheduler multi_step --log_freq 10 --device cuda --optimizer prox_sgd --seed 1234 --verbose 1

# Run pFedME
echo "Run "
python run_experiment.py cifar10 pFedMe --n_learners 1 --n_rounds 201 --bz 128 --lr 0.001 --mu 1.0 \
 --lr_scheduler multi_step --log_freq 10 --device cuda --optimizer prox_sgd --seed 1234 --verbose 1

# run FedEM
echo "Run FedEM"
python run_experiment.py cifar10 FedEM --n_learners 3 --n_rounds 201 --bz 128 --lr 0.03 \
 --lr_scheduler multi_step --log_freq 10 --device cuda --optimizer sgd --seed 1234 --verbose 1
```

Similar for other datasets are provided in `papers_experiments/`

### Effect of M

Run the following scripts to get the logs corresponding to different choices for the parameter M (here we ive an example with M=4)

```effect_of_M
# run FedEM

echo "Run FedEM | M=4"
python run_experiment.py cifar10 FedEM \
    --n_learners 4 \
    --n_rounds 201 \
    --bz 128 --lr 0.03 \
    --lr_scheduler multi_step \
    --log_freq 10  \
    --device cuda \
    --optimizer sgd \
    --logs_root logs/cifar10/FedEM_4 \
    --seed 1234 \
    --verbose 1
```

### Generalization to unseen clients

You need to run the same script as in the previous section. Make sure that `--test-clients-frac` is non-zero,
when you call `generate_data.py`.

### Clients sampling

Our code gives the possibility to use only a fraction of the available clients at each round, you can specify this  parameter when running `run_experiment.py` using the argument `--sampling_rate` (default is `0`). 

### Fully-decentralized federated learning

To simulate a fully-decentralized training you need to specify `--decentralized` when you run `run_experiment.py`

## Results

The  performance of each personalized model (which is the same for all clients  in the case of FedAvg and FedProx) is evaluated on the local test dataset (unseen at training). The following shows the average weighted accuracy with  weights proportional to local dataset sizes. We observe that FedEM obtains  the best performance across all datasets.

每个个性化模型的性能（在 FedAvg 和 FedProx 的情况下对所有客户端都是相同的）在本地测试数据集（在训练中看不到）上进行评估。 下面显示了权重与本地数据集大小成比例的平均加权精度。 我们观察到 FedEM 在所有数据集中获得了最佳性能。
|Dataset         | Local | FedAvg| FedAvg+ | FedEM (Ours)|
| ------------------  |  ------|-------|----------------       | --------------         |
| FEMNIST   |     71.0       |     78.6              |75.3        | 79.9|
| EMNIST    |    71.9     |      82.6              |83.1          |83.5|
| CIFAR10   |     70.2        |      78.2             |82.3          |84.3|
| CIFAR100    |     31.5        |      40.9              |39.0         |44.1|
| Shakespeare |     32.0        |      46.7              |    40.0      |43.7|


We can also visualise the evolution of the train loss, train accuracy, test loss and test accuracy for CIFAR-10 dataset

![](https://user-images.githubusercontent.com/42912620/120851258-de39a800-c578-11eb-8d0e-13460e5d71cc.PNG)

Similar plots can be built for other experiments using the `make_plot` function in `utils/plots.py`
